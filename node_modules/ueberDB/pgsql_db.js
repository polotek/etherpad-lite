/**
 * 2011 Marco Rogers - Yammer Inc.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *      http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS-IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

var util = require('util')
  , events = require('events')
  , async = require('async')
  , pg = require("pg");

 var log4js = require('log4js');

 var ueberLog = log4js.getLogger('ueberDB');
//if(pg.native) pg = pg.native;

// TODO: wtf, need a proper upsert
// http://www.postgresql.org/docs/current/static/plpgsql-control-structures.html#PLPGSQL-UPSERT-EXAMPLE
var upsertFunc =  '' +
'CREATE FUNCTION upsert_key(_key VARCHAR, data TEXT, padID VARCHAR, rev INT, last_updated TIMESTAMP WITH TIME ZONE, _network_id BIGINT) RETURNS VOID AS \n' +
'$$ \n' +
'BEGIN \n' +
'    LOOP \n' +
'        -- first try to update the key \n' +
'        UPDATE "store" SET "value" = data, "pad_id" = padID, "rev_id" = rev, "timestamp" = last_updated, "network_id" = _network_id WHERE "key" = _key; \n' +
'        IF found THEN \n' +
'            RETURN; \n' +
'        END IF; \n' +
'        -- not there, so try to insert the key \n' +
'        -- if someone else inserts the same key concurrently, \n' +
'        -- we could get a unique-key failure \n' +
'        BEGIN \n' +
'            INSERT INTO "store" ("key", "value", "pad_id", "rev_id", "timestamp", "network_id") VALUES (_key, data, padID, rev, last_updated, _network_id); \n' +
'            RETURN; \n' +
'        EXCEPTION WHEN unique_violation THEN \n' +
'            -- do nothing, and loop to try the UPDATE again \n' +
'        END; \n' +
'    END LOOP; \n' +
'END; \n' +
'$$ \n' +
'LANGUAGE plpgsql;';

var createQuery = {
    name: 'create'
    , text: 'CREATE TABLE "store" ( ' +
        '"key" VARCHAR( 100 ) NOT NULL PRIMARY KEY,' + 
        '"value" TEXT NOT NULL, ' +
        '"pad_id" VARCHAR( 100 ), ' +
        '"rev_id" INT, ' +
        '"timestamp" TIMESTAMP WITH TIME ZONE, ' +
        '"network_id" BIGINT' +
        ' );'
  }
  , getQuery = {
    name: 'get'
    , text:'SELECT "value" FROM "store" WHERE "key" = $1;'
  }
  , setQuery = {
    name: 'set'
    , text: 'SELECT upsert_key($1, $2, $3, $4, $5, $6);'
  }
  , removeQuery = {
    name: 'remove'
    , text: 'DELETE FROM "store" WHERE "key" = $1;'
  }

exports.database = function(settings)
{
  events.EventEmitter.apply(this, arguments);

  this.db = new pg.Client(settings);
  this.settings = settings;

  this.settings.cache = 1000;
  this.settings.writeInterval = 100;
  this.settings.json = true;
}
util.inherits(exports.database, events.EventEmitter);

exports.database.prototype.init = function(callback)
{
  var self = this
    , connectionErrHandler = function(err) {
        self.db.end();
        if(self.db.stream) { self.db.stream.destroy(); }
        self.db = new pg.Client(self.settings);
        callback && callback(err);
      }

  self.db.connect();
  self.db.on('error', connectionErrHandler);
  self.db.once('connect', function(err) {
    self.db.removeListener('error', connectionErrHandler);

    if(err) {
      return callback && callback(err);
    }

    var checkErrors = function (cb) {
      // These structures may already be there, so ignore that error,
      // return anything else
      return function(err) {
        if(err && (/already exists/i).test(err.message)) {
          err = null;
        }
        cb(err);
      }
    }

    async.waterfall([
      function(cb) {
        self.db.query(upsertFunc, checkErrors(cb));
      }
      , function(cb) {
        self.db.query(createQuery, checkErrors(cb));
      }
    ]
    , callback);
  });
}

exports.database.prototype.get = function (key, callback)
{
  var self = this
    , starttime = (new Date()).getTime();

  self.db.query(getQuery, [key], function(err,results)
  {
    var value = null;
   
    if(!err && results.rows.length == 1)
    {
      value = results.rows[0].value;
    }
    self.emit('metric.get', (new Date()).getTime() - starttime);
    callback(err,value);
  });
}

exports.database.prototype.set = function (key, value, callback)
{
  var self = this
    , starttime = (new Date()).getTime();

  if(key.length > 100) {
    return callback(new Error("Your Key can only be 100 chars"));
  }

  // Careful! Ordering of key and value is reversed in sql
  var columnData = getColumnData( key, value);
  self.db.query(setQuery, [key, value].concat(columnData), function () {
    self.emit('metric.set', (new Date()).getTime() - starttime);
    callback.apply(this, arguments);
  });

}

exports.database.prototype.remove = function (key, callback)
{
  var self = this
    , starttime = (new Date()).getTime();

  self.db.query(removeQuery, [key], function () {
    self.emit('metric.remove', (new Date()).getTime() - starttime);
    callback.apply(this, arguments);
  });
}

exports.database.prototype.doBulk = function (bulk, callback)
{ 
  var self = this
    , bulkStarttime = (new Date()).getTime()
    , breakLoop = false
    , handle = function(err) {
      if(err) {
        breakLoop = err;
        self.db.query('ROLLBACK;');
        self.db.resumeDrain();
        return err;
      }

      return false;
    }

  self.db.pauseDrain();
  self.db.query('BEGIN;');

  bulk.forEach(function(op, i) {
    if(breakLoop) { return; }

    var starttime = (new Date()).getTime();

    if(op.type == "set")
    {
      var columnData = getColumnData( op.key, op.value );
      self.db.query(setQuery, [op.key, op.value].concat( columnData ), function(err) {
        if(handle(err)) { return; }
        self.emit('metric.set', (new Date()).getTime() - starttime);
      });
    }
    else if(op.type == "remove")
    {
      self.db.query(removeQuery, [op.key], function(err) {
        if(handle(err)) { return; }

        self.emit('metric.remove', (new Date()).getTime() - starttime);
      });
    }
  });

  // breakLoop contains any errors thrown during the transaction
  if(breakLoop) {
    callback(breakLoop);
  } else {
    self.db.query('COMMIT;', function(err) {
      self.emit('metric.bulk', (new Date()).getTime() - bulkStarttime);
      self.db.resumeDrain();
      callback(breakLoop);
    });
  }
}

function getColumnData( key, value )
{
  var pad_id = null
    , rev_id = null
    , timestamp = null
    , network_id = null;
  
  try
  {
    var keyList = key.split(':');
    var keyLength = keyList.length;
    

    if(keyLength > 0 && keyList[0] == 'pad')
    {
      // TODO:mpatel This is bad. Every set call that qualifies does JSON parsing.
      // Need to make changes to underlying CacheAndBufferLayer.  
      var valueAsJson = JSON.parse(value);
      pad_id = keyList[1];
      
      if(keyLength == 2)
      {
        rev_id = parseInt(valueAsJson.head);
        network_id = parseInt(valueAsJson.networkId);
      }
      else if( keyLength >= 4 && keyList[2] == 'revs')
      {
        rev_id = parseInt(keyList[3]);
        timestamp = new Date(valueAsJson.meta.timestamp);
      }
    }
    return [pad_id, rev_id, timestamp, network_id];
  }
  catch(e)
  {
    ueberLog.error(e.message);
    // If there is any error parsing or something, we return all
    // the values as NULL to avoid any weird datatype entering 
    // DB and blowing it up.
    return[null, null, null, null];
  }
}


exports.database.prototype.close = function(callback)
{
  this.db.end();
  if(callback) callback();
}
